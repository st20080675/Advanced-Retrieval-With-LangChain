{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/st20080675/Advanced-Retrieval-With-LangChain/blob/main/Advanced_Retrieval_With_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4dbe2e3",
      "metadata": {
        "id": "f4dbe2e3"
      },
      "source": [
        "# Advanced Retrieval With LangChain\n",
        "\n",
        "Let's go over a few more complex and advanced retrieval methods with LangChain.\n",
        "\n",
        "There is no one right way to retrieve data - it'll depend on your application so take some time to think about it before you jump in\n",
        "\n",
        "Let's have some fun\n",
        "\n",
        "* **Multi Query** - Given a single user query, use an LLM to synthetically generate multiple other queries. Use each one of the new queries to retrieve documents, take the union of those documents for the final context of your prompt\n",
        "* **Contextual Compression** - Fluff remover. Normal retrieval but with an extra step of pulling out relevant information from each returned document. This makes each relevant document smaller for your final prompt (which increases information density)\n",
        "* **Parent Document Retriever** - Split and embed *small* chunks (for maximum information density), then return the parent documents (or larger chunks) those small chunks come from\n",
        "* **Ensemble Retriever** - Combine multiple retrievers together\n",
        "* **Self-Query** - When the retriever infers filters from a users query and applies those filters to the underlying data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf93b72",
      "metadata": {
        "id": "dcf93b72"
      },
      "outputs": [],
      "source": [
        "# from dotenv import load_dotenv\n",
        "# import os\n",
        "\n",
        "# load_dotenv()\n",
        "\n",
        "# openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50493ba9",
      "metadata": {
        "id": "50493ba9"
      },
      "source": [
        "## Load up our texts and documents\n",
        "\n",
        "Then chunk them, and put them into a vector store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QJEwfVwouSF",
        "outputId": "39f4a07e-317f-44db-c7a2-606dff60a4f9"
      },
      "id": "4QJEwfVwouSF",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/809.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/809.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/809.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m809.0/809.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff07d69b",
      "metadata": {
        "id": "ff07d69b"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49749eb",
      "metadata": {
        "id": "d49749eb"
      },
      "source": [
        "We're going to load up Paul Graham's essays. In this repo there are various sizes of folders (`PaulGrahamEssaysSmall`, `PaulGrahamEssaysMedium`, `PaulGrahamEssaysLarge` or `PaulGrahamEssays` for the full set.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download data from [here](https://github.com/gkamradt/langchain-tutorials/tree/main/data/PaulGrahamEssaysLarge)  first. regarding how to download a subfolder content from a git repos, see [here](https://stackoverflow.com/questions/7106012/download-a-single-folder-or-directory-from-a-github-repo/38879691#38879691)"
      ],
      "metadata": {
        "id": "EMEspK82soA8"
      },
      "id": "EMEspK82soA8"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "06DpOzgHuY6E",
        "outputId": "4cd6a61b-3138-454c-81cb-c79da2cb316c"
      },
      "id": "06DpOzgHuY6E",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.12.6-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff==2.2.1 (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: certifi==2024.2.2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.2.2)\n",
            "Requirement already satisfied: chardet==5.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from unstructured) (8.1.7)\n",
            "Requirement already satisfied: dataclasses-json==0.6.4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.4)\n",
            "Collecting dataclasses-json-speakeasy==0.5.11 (from unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Collecting emoji==2.10.1 (from unstructured)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filetype==1.2.0 (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: idna==3.6 in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.6)\n",
            "Requirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.3.2)\n",
            "Collecting jsonpath-python==1.0.6 (from unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting langdetect==1.0.9 (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lxml==5.1.0 (from unstructured)\n",
            "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow==3.20.2 (from unstructured)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.0)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Collecting numpy==1.26.4 (from unstructured)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (23.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.8.2)\n",
            "Collecting python-iso639==2024.2.7 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-magic==0.4.27 (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Collecting rapidfuzz==3.6.1 (from unstructured)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex==2023.12.25 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2023.12.25)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: soupsieve==2.5 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.5)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.2)\n",
            "Collecting typing-extensions==4.9.0 (from unstructured)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: typing-inspect==0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Collecting unstructured-client==0.18.0 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Collecting urllib3==1.26.18 (from unstructured)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt==1.16.0 (from unstructured)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=aa1e78bee3e1a6d4f1fbfa374ce7b9322e0bc9c5e802fd880c2abf10fe21e9c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, wrapt, urllib3, typing-extensions, rapidfuzz, python-magic, python-iso639, numpy, marshmallow, lxml, langdetect, jsonpath-python, emoji, backoff, dataclasses-json-speakeasy, unstructured-client, unstructured\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: marshmallow\n",
            "    Found existing installation: marshmallow 3.21.1\n",
            "    Uninstalling marshmallow-3.21.1:\n",
            "      Successfully uninstalled marshmallow-3.21.1\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.4\n",
            "    Uninstalling lxml-4.9.4:\n",
            "      Successfully uninstalled lxml-4.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 dataclasses-json-speakeasy-0.5.11 emoji-2.10.1 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 lxml-5.1.0 marshmallow-3.20.2 numpy-1.26.4 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.6.1 typing-extensions-4.9.0 unstructured-0.12.6 unstructured-client-0.18.0 urllib3-1.26.18 wrapt-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              },
              "id": "b70a2852a7514b0aaa9c2c68677a955c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ae08f45b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae08f45b",
        "outputId": "668ecc6a-b41b-449c-ccfd-23a6b3228a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49/49 [00:24<00:00,  1.99it/s]\n"
          ]
        }
      ],
      "source": [
        "# loader = DirectoryLoader('../data/PaulGrahamEssaysLarge/', glob=\"**/*.txt\", show_progress=True)\n",
        "loader = DirectoryLoader('/content/PaulGrahamEssaysLarge/', glob=\"**/*.txt\", show_progress=True)\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8449d623",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8449d623",
        "outputId": "bbf151ff-6207-4c76-865a-18fa9e36d891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 49 essays loaded\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(docs)} essays loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ca003e",
      "metadata": {
        "id": "e0ca003e"
      },
      "source": [
        "Then we'll split up our text into smaller sized chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ddd71b36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd71b36",
        "outputId": "cd979ec2-6cc3-4dcd-f028-1d3a1b9c178a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your 49 documents have been split into 468 chunks\n"
          ]
        }
      ],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print (f\"Your {len(docs)} documents have been split into {len(splits)} chunks\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4fb96a95",
      "metadata": {
        "id": "4fb96a95"
      },
      "source": [
        "If you do `Chroma.from_documents` multiple times you'll re-add the documents (and duplicate them) which is annoying. I check to see if we've already made our vectordb, if so delete what's in there, then go and make it"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install faiss-cpu\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muS-iQXkv3cu",
        "outputId": "cb4c180c-a4b4-44a7-d4f5-b25565d4f96e"
      },
      "id": "muS-iQXkv3cu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.1.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.4)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.3.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.9.15)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.0-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=98cabb198daf963ff997faee72158e949103b3d1818c87d810f7b7c53d32d58b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.0.2\n",
            "    Uninstalling importlib_metadata-7.0.2:\n",
            "      Successfully uninstalled importlib_metadata-7.0.2\n",
            "Successfully installed asgiref-3.8.0 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.0 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-api-1.23.0 opentelemetry-exporter-otlp-proto-common-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.23.0 opentelemetry-instrumentation-0.44b0 opentelemetry-instrumentation-asgi-0.44b0 opentelemetry-instrumentation-fastapi-0.44b0 opentelemetry-proto-1.23.0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 opentelemetry-util-http-0.44b0 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# embedding = OpenAIEmbeddings()\n",
        "embedding = HuggingFaceInstructEmbeddings()\n",
        "\n",
        "if 'vectordb' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
        "    vectordb.delete_collection()\n",
        "\n",
        "# the follwing line took 44m for me, suggest using a subset of the doc\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "932Kzu0rvrZ2",
        "outputId": "0c380499-6315-4d0c-c1e0-04e929879868"
      },
      "id": "932Kzu0rvrZ2",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38be1d0e",
      "metadata": {
        "id": "38be1d0e"
      },
      "source": [
        "### MultiQuery\n",
        "\n",
        "This retrieval method will generated 3 additional questions to get a total of 4 queries (with the users included) that will be used to go retrieve documents. This is helpful when you want to retrieve documents which are similar in meaning to your question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fef59032",
      "metadata": {
        "id": "fef59032"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Set logging for the queries\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adc71aa",
      "metadata": {
        "id": "2adc71aa"
      },
      "source": [
        "Doing some logging to see the other questions that were generated. I tried to find a way to get these via a model property but couldn't, lmk if you find a way!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ed315ac4",
      "metadata": {
        "id": "ed315ac4"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0349ab",
      "metadata": {
        "id": "bf0349ab"
      },
      "source": [
        "Then we set up the MultiQueryRetriever which will generate other questions for us"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this openai version give me: RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota\n",
        "\n",
        "# !pip install openai\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# openai_api_key = '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'\n",
        "# llm = ChatOpenAI(openai_api_key = openai_api_key, temperature=0)\n",
        "# question = \"What is the authors view on the early stages of a startup?\"\n",
        "# retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "#     retriever=vectordb.as_retriever(), llm=llm\n",
        "# )"
      ],
      "metadata": {
        "id": "KCGSsXXcEYGZ"
      },
      "id": "KCGSsXXcEYGZ",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4c9750dc",
      "metadata": {
        "id": "4c9750dc"
      },
      "outputs": [],
      "source": [
        "question = \"What is the authors view on the early stages of a startup?\"\n",
        "# llm = ChatOpenAI(temperature=0)\n",
        "from langchain import HuggingFaceHub\n",
        "import os\n",
        "\n",
        "huggingface_api_key = \"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\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_api_key\n",
        "\n",
        "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.7, \"max_length\":512})\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c7a55b41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a55b41",
        "outputId": "bbce9525-1bc4-4633-fb99-d7ba4bb2ac36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"What is the author's view on the early stages of a startup?\"]\n"
          ]
        }
      ],
      "source": [
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec261df",
      "metadata": {
        "id": "eec261df"
      },
      "source": [
        "Check out how there are other questions which are related to but slightly different than the question I asked.\n",
        "\n",
        "Let's see how many docs were actually returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "afd3449d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afd3449d",
        "outputId": "2e3a94d4-4e8f-426c-b2ec-b5f42d3fc253"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(unique_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914570e9",
      "metadata": {
        "id": "914570e9"
      },
      "source": [
        "Ok now let's put those docs into a prompt template which we'll use as context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4669bc0e",
      "metadata": {
        "id": "4669bc0e"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "240b7b56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "240b7b56",
        "outputId": "cd01e5a4-fd51-497d-d832-1e64cca5076e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"get a version 1 out fast, then improve it based on users' reactions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# llm.predict(text=PROMPT.format_prompt(\n",
        "#     context=unique_docs,\n",
        "#     question=question\n",
        "# ).text)\n",
        "\n",
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=unique_docs[:1],\n",
        "    question=question\n",
        ").text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff39008",
      "metadata": {
        "id": "eff39008"
      },
      "source": [
        "### Contextual Compression\n",
        "\n",
        "Then we'll move onto contextual compression. This will take the chunk that you've made (above) and compress it's information down to the parts relevant to your query.\n",
        "\n",
        "Say that you have a chunk that has 3 topics within it, you only really care about one of them though, this compressor will look at your query, see that you only need one of the 3 topics, then extract & return that one topic.\n",
        "\n",
        "This one is a bit more expensive because each doc returned will get processed an additional time (to pull out the relevant data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f1efc668",
      "metadata": {
        "id": "f1efc668"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f073d1d",
      "metadata": {
        "id": "9f073d1d"
      },
      "source": [
        "We first need to set up our compressor, it's cool that it's a separate object because that means you can use it elsewhere outside this retriever as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7eb5c6e5",
      "metadata": {
        "id": "7eb5c6e5"
      },
      "outputs": [],
      "source": [
        "# llm = ChatOpenAI(temperature=0, model='gpt-4')\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                       base_retriever=vectordb.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc392b6",
      "metadata": {
        "id": "fdc392b6"
      },
      "source": [
        "First, an example of compression. Below we have one of our splits that we made above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1561a033",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1561a033",
        "outputId": "a13de1d5-5f70-4437-f9e2-088cc7863c36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aaron Swartz created a scraped\\n\\nfeed\\n\\nof the essays page.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "splits[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab86de32",
      "metadata": {
        "id": "ab86de32"
      },
      "source": [
        "Now we are going to pass a question to it and with that question we will compress the doc. The cool part is this doc will be contextually compressed, meaning the resulting file will only have the information relevant to the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "40ebdfd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ebdfd6",
        "outputId": "8962bea1-880a-46e8-c905-21b7a1a6af27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Aaron Swartz created a scraped feed', metadata={'source': '/content/PaulGrahamEssaysLarge/rss.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "compressor.compress_documents(documents=[splits[0]], query=\"test for what you like to do\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95a4c1c",
      "metadata": {
        "id": "c95a4c1c"
      },
      "source": [
        "Great so we had a long document, now we have a shorter document with more dense information. Great for getting rid of the fluff. Let's try it out on our essays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "51c4daee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51c4daee",
        "outputId": "04a8fb94-9176-46ac-f6c3-7eae2ffb3cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the authors view on the early stages of a startup?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "83cb292f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83cb292f",
        "outputId": "5a2f120d-4444-474f-a349-a4f39f2eda23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"1. Release Early.The thing I probably repeat most is this recipe for a startup: get a version 1 out fast, then improve it based on users' reactions.\", metadata={'source': '/content/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
              " Document(page_content=\"Startups are very counterintuitive. I'm not sure why. Maybe it's just because knowledge about them hasn't permeated our culture yet. But whatever the reason, starting a startup is a task where you can't always trust your instincts.\", metadata={'source': '/content/PaulGrahamEssaysLarge/before.txt'}),\n",
              " Document(page_content=\"Almost everyone's initial plan is broken. If companies stuck to their initial plans, Microsoft would be selling programming languages, and Apple would be selling printed circuit boards. In both cases their customers told them what their business should be-- and they were smart enough to listen.\", metadata={'source': '/content/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
              " Document(page_content=\"it's gratuitously stupid to do that at 20. TryShould you do it at any age? I realize I've made startups sound pretty hard. If I haven't, let me try again: starting a startup is really hard. What if it's too hard? How can you tell if you're up to this challenge? The answer is the fifth counterintuitive point: you can't tell.\", metadata={'source': '/content/PaulGrahamEssaysLarge/before.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "print (len(compressed_docs))\n",
        "compressed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43bb531c",
      "metadata": {
        "id": "43bb531c"
      },
      "source": [
        "We now have 4 docs but they are shorter and only contain the information that is relevant to our query.\n",
        "\n",
        "Let's put it in our prompt template again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e08044f7",
      "metadata": {
        "id": "e08044f7"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "b174f262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b174f262",
        "outputId": "e350c950-e377-40af-a697-428486475e87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'They are counterintuitive and often fail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=compressed_docs,\n",
        "    question=question\n",
        ").text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98d7d51f",
      "metadata": {
        "id": "98d7d51f"
      },
      "source": [
        "### Parent Document Retriever\n",
        "\n",
        "[LangChain documentation](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever) does a great job describing this - my minor edits below:\n",
        "\n",
        "When you split your docs, you generally may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
        "\n",
        "But at the same time you may want to have information around those small chunks to keep context of the longer document.\n",
        "\n",
        "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
        "\n",
        "Note that \"parent document\" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "3fd6e470",
      "metadata": {
        "id": "3fd6e470"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "503444a8",
      "metadata": {
        "id": "503444a8"
      },
      "outputs": [],
      "source": [
        "# This text splitter is used to create the child documents. They should be small chunk size.\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e1f3d1c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f3d1c4",
        "outputId": "d6ecf005-e8d4-43cf-f85a-8744c81d4cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "\n",
        "# vectorstore = Chroma(\n",
        "#     collection_name=\"return_full_documents\",\n",
        "#     embedding_function=OpenAIEmbeddings()\n",
        "# )\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"return_full_documents\",\n",
        "    embedding_function=HuggingFaceInstructEmbeddings()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2d98928b",
      "metadata": {
        "id": "2d98928b"
      },
      "outputs": [],
      "source": [
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e896018a",
      "metadata": {
        "id": "e896018a"
      },
      "source": [
        "Now we will add the whole essays that we split above. We haven't chunked these essays yet, but the `.add_documents` will do the small chunking for us with the `child_splitter` above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "098bf4a3",
      "metadata": {
        "id": "098bf4a3"
      },
      "outputs": [],
      "source": [
        "# this line of code took 1h 12m\n",
        "retriever.add_documents(docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078d2854",
      "metadata": {
        "id": "078d2854"
      },
      "source": [
        "Now if we were to put in a question or query, we'll get small chunks returned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "a3ad876b",
      "metadata": {
        "id": "a3ad876b"
      },
      "outputs": [],
      "source": [
        "sub_docs = vectorstore.similarity_search(\"what is some investing advice?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ddc663c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddc663c8",
        "outputId": "9c648abf-e70c-4865-de64-5e648aeee6e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"advising at Y Combinator, I would have said: Stop being so stressed\\n\\nout, because you're doing fine. You're growing 7x a year. Just don't\\n\\nhire too many more people and you'll soon be profitable, and then\\n\\nyou'll control your own destiny.Alas I hired lots more people, partly because our investors wanted\\n\\nme to, and partly because that's what startups did during the\", metadata={'doc_id': '4a66829a-c7d3-4274-88de-e5afa891ab56', 'source': '/content/PaulGrahamEssaysLarge/worked.txt'}),\n",
              " Document(page_content=\"pay attention. Anyone who's been here any amount of time knows not\\n\\nto default to skepticism, no matter how inexperienced you seem or\\n\\nhow unpromising your idea sounds at first, because they've all seen\\n\\ninexperienced founders with unpromising sounding ideas who a few\\n\\nyears later were billionaires.Having people around you care about what you're doing is an\", metadata={'doc_id': '1431e978-7c0a-4c3f-99eb-37289e711294', 'source': '/content/PaulGrahamEssaysLarge/hubs.txt'}),\n",
              " Document(page_content='motive toward the end of the process.So here is the ultimate advice for young would-be startup founders,\\n\\nboiled down to two words: just learn.\\n\\nNotes[1]\\n\\nSome founders listen more than others, and this tends to be a\\n\\npredictor of success. One of the things I\\n\\nremember about the Airbnbs during YC is how intently they listened. [2]\\n\\nIn fact, this is one of the reasons startups are possible. If', metadata={'doc_id': 'd708c8a3-d4db-4925-a0be-f82fe41ee071', 'source': '/content/PaulGrahamEssaysLarge/before.txt'}),\n",
              " Document(page_content=\"be pushing the limits of whatever you're doing. So things don't\\n\\nhappen in the smooth, predictable way they do in the rest of the\\n\\nworld. Things change suddenly, and usually for the worse.Shielding your optimism is nowhere more important than with deals.\\n\\nIf your startup is doing a deal, just assume it's not going to\\n\\nhappen. The VCs who say they're going to invest in you aren't.\", metadata={'doc_id': 'fd6c596a-94fb-42d9-8877-0e3890fe3f38', 'source': '/content/PaulGrahamEssaysLarge/startuplessons.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "sub_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f18f1",
      "metadata": {
        "id": "6b1f18f1"
      },
      "source": [
        "Look how small those chunks are. Now we want to get the parent doc which those small docs are a part of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "17413045",
      "metadata": {
        "id": "17413045"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\"what is some investing advice?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5452837f",
      "metadata": {
        "id": "5452837f"
      },
      "source": [
        "I'm going to only do the first doc to save space, but there are more waiting for you. Keep in mind that LangChain will do the union of docs, so if you have two child docs from the same parent doc, you'll only return the parent doc once, not twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "8ecf6116",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "8ecf6116",
        "outputId": "73da0fe5-824a-44e7-acf1-47f6875aa4bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'February 2021Before college the two main things I worked on, outside of school,\\n\\nwere writing and programming. I didn\\'t write essays. I wrote what\\n\\nbeginning writers were supposed to write then, and probably still\\n\\nare: short stories. My stories were awful. They had hardly any plot,\\n\\njust characters with strong feelings, which I imagined made them\\n\\ndeep.The first programs I tried writing were on the IBM 1401 that our\\n\\nschool district used for what was then called \"data processing.\"\\n\\nThis was in 9th grade, so I was 13 or 14. The school district\\'s\\n\\n1401 happened to be in the basement of our junior high school, and\\n\\nmy friend Rich Draves and I got permission to use it. It was like\\n\\na mini Bond villain\\'s lair down there, with all these alien-looking\\n\\nmachines \\x97 CPU, disk drives, printer, card reader \\x97 sitting up\\n\\non a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to\\n\\ntype programs on punch cards, then stack them in the card reade'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "retrieved_docs[0].page_content[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe9e6ab8",
      "metadata": {
        "id": "fe9e6ab8"
      },
      "source": [
        "However here we got the full document back. Sometimes this will be too long and we actually just want to get a larger chunk instead. Let's do that.\n",
        "\n",
        "Notice the chunk size difference between the parent splitter and child splitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "f6c3d87b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6c3d87b",
        "outputId": "edc3b1c3-bc5e-4831-dbc6-d27e3d2ed36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "# This text splitter is used to create the parent documents\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "\n",
        "# This text splitter is used to create the child documents\n",
        "# It should create documents smaller than the parent\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "# vectorstore = Chroma(collection_name=\"return_split_parent_documents\", embedding_function=OpenAIEmbeddings())\n",
        "vectorstore = Chroma(collection_name=\"return_split_parent_documents\", embedding_function=HuggingFaceInstructEmbeddings())\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "973d4514",
      "metadata": {
        "id": "973d4514"
      },
      "source": [
        "This will set up our retriever for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "aacf5776",
      "metadata": {
        "id": "aacf5776"
      },
      "outputs": [],
      "source": [
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2204ee45",
      "metadata": {
        "id": "2204ee45"
      },
      "source": [
        "Now this time when we add documents two things will happen\n",
        "1. Larger chunks - We'll split our docs into large chunks\n",
        "2. Smaller chunks - We'll split our docs into smaller chunks\n",
        "\n",
        "Both of them will be combined."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95P2U3NqdLsp",
        "outputId": "ac84cbd9-2239-4ecb-f024-013c74227594"
      },
      "id": "95P2U3NqdLsp",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "561c9cb6",
      "metadata": {
        "id": "561c9cb6"
      },
      "outputs": [],
      "source": [
        "# retriever.add_documents(docs)\n",
        "\n",
        "# the below line took 3m\n",
        "retriever.add_documents(docs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4bcede3",
      "metadata": {
        "id": "c4bcede3"
      },
      "source": [
        "Let's check out how many documents we have now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "f5e4718d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5e4718d",
        "outputId": "75de7a93-12fe-4886-8f4b-2fbb0c8f3f11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "len(list(store.yield_keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11327641",
      "metadata": {
        "id": "11327641"
      },
      "source": [
        "Then let's go get our small chunks to make sure it's working and see how long they are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "7e6f6150",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e6f6150",
        "outputId": "ba5f17ae-bf23-4337-c4c9-1d3700d4f9bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"them to make their own investment decisions. Most are only allowed\\n\\nto invest in deals where some reputable private VC firm is willing\\n\\nto act as lead investor.Not BuildingsIf you go to see Silicon Valley, what you'll see are buildings.\\n\\nBut it's the people that make it Silicon Valley, not the buildings.\", metadata={'doc_id': '410337f2-8e9e-402f-a8c7-0618a1f88b6f', 'source': '/content/PaulGrahamEssaysLarge/siliconvalley.txt'}),\n",
              " Document(page_content=\"there's no one to invest in them.Not BureaucratsDo you really need the rich people? Wouldn't it work to have the\\n\\ngovernment invest in the nerds? No, it would not. Startup investors\\n\\nare a distinct type of rich people. They tend to have a lot of\\n\\nexperience themselves in the technology business. This (a) helps\\n\\nthem pick the right startups, and (b) means they can supply advice\", metadata={'doc_id': '410337f2-8e9e-402f-a8c7-0618a1f88b6f', 'source': '/content/PaulGrahamEssaysLarge/siliconvalley.txt'}),\n",
              " Document(page_content='studies of returns from startup investing, which is all about\\n\\nhitting outliers, are not one of them.\\n\\nThanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading\\n\\ndrafts of this.', metadata={'doc_id': '189b374d-aae6-4b28-80e1-7448c62d4c9b', 'source': '/content/PaulGrahamEssaysLarge/bias.txt'}),\n",
              " Document(page_content='are a distinct type of rich people. They tend to have a lot of\\n\\nexperience themselves in the technology business. This (a) helps\\n\\nthem pick the right startups, and (b) means they can supply advice\\n\\nand connections as well as money. And the fact that they have a\\n\\npersonal stake in the outcome makes them really pay attention.Bureaucrats by their nature are the exact opposite sort of people', metadata={'doc_id': '410337f2-8e9e-402f-a8c7-0618a1f88b6f', 'source': '/content/PaulGrahamEssaysLarge/siliconvalley.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "sub_docs = vectorstore.similarity_search(\"what is some investing advice?\")\n",
        "sub_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa3cffa",
      "metadata": {
        "id": "6fa3cffa"
      },
      "source": [
        "Now, let's do the full process, we'll see what small chunks are generated, but then return the larger chunks as our relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "db8e4809",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db8e4809",
        "outputId": "0feff213-103e-44fc-bd39-35e059613412"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"list, the University of Washington yielded a high-tech community\\n\\nin Seattle, and the University of Texas at Austin yielded one in\\n\\nAustin. But what happened in Pittsburgh? And in Ithaca, home of\\n\\nCornell, which is also high on the list?I grew up in Pittsburgh and went to college at Cornell, so I can\\n\\nanswer for both. The weather is terrible,  particularly in winter,\\n\\nand there's no interesting old city to make up for it, as there is\\n\\nin Boston. Rich people don't want to live in Pittsburgh or Ithaca.\\n\\nSo while there are plenty of hackers who could start startups,\\n\\nthere's no one to invest in them.Not BureaucratsDo you really need the rich people? Wouldn't it work to have the\\n\\ngovernment invest in the nerds? No, it would not. Startup investors\\n\\nare a distinct type of rich people. They tend to have a lot of\\n\\nexperience themselves in the technology business. This (a) helps\\n\\nthem pick the right startups, and (b) means they can supply advice\\n\\nand connections as well as money. And the fact that they have a\\n\\npersonal stake in the outcome makes them really pay attention.Bureaucrats by their nature are the exact opposite sort of people\\n\\nfrom startup investors. The idea of them making startup investments\\n\\nis comic. It would be like mathematicians running Vogue-- or\\n\\nperhaps more accurately, Vogue editors running a math journal.\\n\\n[2]Though indeed, most things bureaucrats do, they do badly. We just\\n\\ndon't notice usually, because they only have to compete against\\n\\nother bureaucrats. But as startup investors they'd have to compete\\n\\nagainst pros with a great deal more experience and motivation.Even corporations that have in-house VC groups generally forbid\\n\\nthem to make their own investment decisions. Most are only allowed\\n\\nto invest in deals where some reputable private VC firm is willing\\n\\nto act as lead investor.Not BuildingsIf you go to see Silicon Valley, what you'll see are buildings.\\n\\nBut it's the people that make it Silicon Valley, not the buildings.\", metadata={'source': '/content/PaulGrahamEssaysLarge/siliconvalley.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "larger_chunk_relevant_docs = retriever.get_relevant_documents(\"what is some investing advice?\")\n",
        "larger_chunk_relevant_docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(larger_chunk_relevant_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaTjG6nBfSDS",
        "outputId": "a6b78786-f782-4d43-e392-8da295b5f428"
      },
      "id": "SaTjG6nBfSDS",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "5689ec55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5689ec55",
        "outputId": "982e7139-0ffd-4f5d-bbd8-ba6f5e2c9c1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Startup investors are a distinct type of rich people. They tend to have a lot of experience themselves in the technology business.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "question = \"what is some investing advice?\"\n",
        "\n",
        "# Input validation error: `inputs` must have less than 1024 tokens. Given: 1146\n",
        "# llm.predict(text=PROMPT.format_prompt(\n",
        "#     context=larger_chunk_relevant_docs,\n",
        "#     question=question\n",
        "# ).text)\n",
        "\n",
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=larger_chunk_relevant_docs[0],\n",
        "    question=question\n",
        ").text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76261c6",
      "metadata": {
        "id": "a76261c6"
      },
      "source": [
        "### Ensemble Retriever\n",
        "\n",
        "The next one on our list combines multiple retrievers together. The goal here is to see what multiple methods return, then pull them together for (hopefully) better results.\n",
        "\n",
        "You may need to install bm25 with `!pip install rank_bm25`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbLIO7E5f_86",
        "outputId": "dd3567cb-b6a5-49ee-9642-4ff800f27a55"
      },
      "id": "tbLIO7E5f_86",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "2d1e83f1",
      "metadata": {
        "id": "2d1e83f1"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21bec867",
      "metadata": {
        "id": "21bec867"
      },
      "source": [
        "We'll use a [BM25 retriever](https://en.wikipedia.org/wiki/Okapi_BM25) for this one which is really good at keyword matching (vs semantic). When you combine this method with regular semantic search it's known as hybrid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "f97add10",
      "metadata": {
        "id": "f97add10"
      },
      "outputs": [],
      "source": [
        "# initialize the bm25 retriever and faiss retriever\n",
        "bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "bm25_retriever.k = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "20b58715",
      "metadata": {
        "id": "20b58715"
      },
      "outputs": [],
      "source": [
        "# you probably has build the 'bectordb' already\n",
        "# embedding = OpenAIEmbeddings()\n",
        "# embedding = HuggingFaceInstructEmbeddings()\n",
        "# vectordb = Chroma.from_documents(splits, embedding)\n",
        "vectordb = vectordb.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "d4eb755d",
      "metadata": {
        "id": "d4eb755d"
      },
      "outputs": [],
      "source": [
        "# initialize the ensemble retriever\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vectordb], weights=[0.5, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "0d38a44a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d38a44a",
        "outputId": "a6dd00e6-3f6e-4bdb-84db-a227f639e86f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "ensemble_docs = ensemble_retriever.get_relevant_documents(\"what is some investing advice?\")\n",
        "len(ensemble_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "f7e930e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "f7e930e4",
        "outputId": "afa7577f-54d5-4598-d71d-4a72e22c775c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I spent almost a decadenninvesting in early stage startups, and curiously enough protectingnnyourself against obsolete beliefs is exactly what you have to donnto succeed as a startup investor. Most really good startup ideasnnlook like bad ideas at first, and many of those look bad specificallynnbecause some change in the world just switched them from bad to good. I spent a lot of time learning to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "question = \"what is some investing advice?\"\n",
        "# Input validation error: `inputs` must have less than 1024 tokens. Given: 1793\n",
        "# llm.predict(text=PROMPT.format_prompt(\n",
        "#     context=ensemble_docs,\n",
        "#     question=question\n",
        "# ).text)\n",
        "\n",
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=ensemble_docs[:2],\n",
        "    question=question\n",
        ").text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5deae6d2",
      "metadata": {
        "id": "5deae6d2"
      },
      "source": [
        "### Self Querying\n",
        "\n",
        "The last one we'll look at today is self querying. This is when the retriever has the ability to query itself. It does this so it can use filters when doing it's final query.\n",
        "\n",
        "This means it'll use the users query for semantic search, but also its own query for filtering (so the user doesn't have to give a structured filter).\n",
        "\n",
        "You may need to install `!pip install lark`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "8308c26b",
      "metadata": {
        "id": "8308c26b"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "\n",
        "# you probably already have 'embedding' and 'llm' in the env\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "# llm = ChatOpenAI(temperature=0, model='gpt-4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f85445b",
      "metadata": {
        "id": "8f85445b"
      },
      "outputs": [],
      "source": [
        "# if 'vectorstore' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
        "#     vectorstore.delete_collection()\n",
        "\n",
        "# vectorstore = Chroma.from_documents(\n",
        "#     splits, embeddings\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2c00190",
      "metadata": {
        "id": "c2c00190"
      },
      "source": [
        "Below is the information on the fitlers available. This will help the model know which filters to semantically search for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "89fdc150",
      "metadata": {
        "id": "89fdc150"
      },
      "outputs": [],
      "source": [
        "metadata_field_info=[\n",
        "    AttributeInfo(\n",
        "        name=\"source\",\n",
        "        description=\"The filename of the essay\",\n",
        "        type=\"string or list[string]\",\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall lark\n",
        "!pip install lark-parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehf6VqAZjvjl",
        "outputId": "073d202c-712e-401f-ce90-85b0791e76e9"
      },
      "id": "ehf6VqAZjvjl",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: lark 1.1.9\n",
            "Uninstalling lark-1.1.9:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/lark-1.1.9.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/lark/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.10/dist-packages/lark/parsers/lalr_puppet.py\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled lark-1.1.9\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.10/dist-packages (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6cxPxU3lqpw",
        "outputId": "d8505342-00aa-4260-aaa2-39fd57071fec"
      },
      "id": "i6cxPxU3lqpw",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lark\n",
            "  Using cached lark-1.1.9-py3-none-any.whl (111 kB)\n",
            "Installing collected packages: lark\n",
            "Successfully installed lark-1.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "898665ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "898665ab",
        "outputId": "3c4831f3-224a-4f6a-c60d-464d46ad9dba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Cannot import lark, please install it with 'pip install lark'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-46c9ff546b49>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocument_content_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Essays from Paul Graham\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m retriever = SelfQueryRetriever.from_llm(llm,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                         \u001b[0mvectorstore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                         \u001b[0mdocument_content_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                         \u001b[0mmetadata_field_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/retrievers/self_query/base.py\u001b[0m in \u001b[0;36mfrom_llm\u001b[0;34m(cls, llm, vectorstore, document_contents, metadata_field_info, structured_query_translator, chain_kwargs, enable_limit, use_original_query, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;34m\"allowed_operators\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             ] = structured_query_translator.allowed_operators\n\u001b[0;32m--> 244\u001b[0;31m         query_constructor = load_query_constructor_runnable(\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mdocument_contents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/query_constructor/base.py\u001b[0m in \u001b[0;36mload_query_constructor_runnable\u001b[0;34m(llm, document_contents, attribute_info, examples, allowed_comparators, allowed_operators, enable_limit, schema_prompt, fix_invalid, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mainfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeInfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mainfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[0;32m--> 361\u001b[0;31m     output_parser = StructuredQueryOutputParser.from_components(\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mallowed_comparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_comparators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mallowed_operators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_operators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/query_constructor/base.py\u001b[0m in \u001b[0;36mfrom_components\u001b[0;34m(cls, allowed_comparators, allowed_operators, allowed_attributes, fix_invalid)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             ast_parse = get_parser(\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mallowed_comparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_comparators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mallowed_operators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_operators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/query_constructor/parser.py\u001b[0m in \u001b[0;36mget_parser\u001b[0;34m(allowed_comparators, allowed_operators, allowed_attributes)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# QueryTransformer is None when Lark cannot be imported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mQueryTransformer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;34m\"Cannot import lark, please install it with 'pip install lark'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         )\n",
            "\u001b[0;31mImportError\u001b[0m: Cannot import lark, please install it with 'pip install lark'.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "document_content_description = \"Essays from Paul Graham\"\n",
        "retriever = SelfQueryRetriever.from_llm(llm,\n",
        "                                        vectorstore,\n",
        "                                        document_content_description,\n",
        "                                        metadata_field_info,\n",
        "                                        verbose=True,\n",
        "                                        enable_limit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5a7209",
      "metadata": {
        "id": "fc5a7209",
        "outputId": "fa0ae8c1-5e39-4420-cea8-3256af5bd4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query='figure out what you like to do' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='source', value='../data/PaulGrahamEssaysLarge/island.txt') limit=1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"if I could only figure out what.As for books, I know the house would probably have something to\\n\\nread.  On the average trip I bring four books and only read one of\\n\\nthem, because I find new books to read en route.  Really bringing\\n\\nbooks is insurance.I realize this dependence on books is not entirely good—that what\\n\\nI need them for is distraction.  The books I bring on trips are\\n\\noften quite virtuous, the sort of stuff that might be assigned\\n\\nreading in a college class.  But I know my motives aren't virtuous.\\n\\nI bring books because if the world gets boring I need to be able\\n\\nto slip into another distilled by some writer.  It's like eating\\n\\njam when you know you should be eating fruit.There is a point where I'll do without books.  I was walking in\\n\\nsome steep mountains once, and decided I'd rather just think, if I\\n\\nwas bored, rather than carry a single unnecessary ounce.  It wasn't\\n\\nso bad.  I found I could entertain myself by having ideas instead\\n\\nof reading other people's.  If you stop eating jam, fruit starts\\n\\nto taste better.So maybe I'll try not bringing books on some future trip.  They're\\n\\ngoing to have to pry the plugs out of my cold, dead ears, however.\", metadata={'source': '../data/PaulGrahamEssaysLarge/island.txt'})]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.get_relevant_documents(\"Return only 1 essay. What is one thing you can do to figure out what you like to do from source '../data/PaulGrahamEssaysLarge/island.txt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b16b3757",
      "metadata": {
        "id": "b16b3757"
      },
      "source": [
        "It's kind of annoying to have to put in the full file name, a user doesn't want to do that. Let's change `source` to `essay` and the file path w/ the essay name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95dfc489",
      "metadata": {
        "id": "95dfc489"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "for split in splits:\n",
        "    split.metadata['essay'] = re.search(r'[^/]+(?=\\.\\w+$)', split.metadata['source']).group()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646b9fca",
      "metadata": {
        "id": "646b9fca"
      },
      "source": [
        "Ok now that we did that, let's make a new field info config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbf7f6b",
      "metadata": {
        "id": "9bbf7f6b"
      },
      "outputs": [],
      "source": [
        "metadata_field_info=[\n",
        "    AttributeInfo(\n",
        "        name=\"essay\",\n",
        "        description=\"The name of the essay\",\n",
        "        type=\"string or list[string]\",\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a75eea",
      "metadata": {
        "id": "96a75eea"
      },
      "outputs": [],
      "source": [
        "if 'vectorstore' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
        "    vectorstore.delete_collection()\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    splits, embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a6c78c",
      "metadata": {
        "id": "39a6c78c"
      },
      "outputs": [],
      "source": [
        "document_content_description = \"Essays from Paul Graham\"\n",
        "retriever = SelfQueryRetriever.from_llm(llm,\n",
        "                                        vectorstore,\n",
        "                                        document_content_description,\n",
        "                                        metadata_field_info,\n",
        "                                        verbose=True,\n",
        "                                        enable_limit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ab1345",
      "metadata": {
        "id": "88ab1345",
        "outputId": "8d41f992-286e-4cc7-d258-1e88c8648084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "query='investment advice' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='essay', value='worked') limit=1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='should make a larger number of smaller investments instead of a\\n\\nhandful of giant ones, they should be funding younger, more technical\\n\\nfounders instead of MBAs, they should let the founders remain as\\n\\nCEO, and so on.One of my tricks for writing essays had always been to give talks.\\n\\nThe prospect of having to stand up in front of a group of people\\n\\nand tell them something that won\\'t waste their time is a great\\n\\nspur to the imagination. When the Harvard Computer Society, the\\n\\nundergrad computer club, asked me to give a talk, I decided I would\\n\\ntell them how to start a startup. Maybe they\\'d be able to avoid the\\n\\nworst of the mistakes we\\'d made.So I gave this talk, in the course of which I told them that the\\n\\nbest sources of seed funding were successful startup founders,\\n\\nbecause then they\\'d be sources of advice too. Whereupon it seemed\\n\\nthey were all looking expectantly at me. Horrified at the prospect\\n\\nof having my inbox flooded by business plans (if I\\'d only known),\\n\\nI blurted out \"But not me!\" and went on with the talk. But afterward\\n\\nit occurred to me that I should really stop procrastinating about\\n\\nangel investing. I\\'d been meaning to since Yahoo bought us, and now\\n\\nit was 7 years later and I still hadn\\'t done one angel investment.Meanwhile I had been scheming with Robert and Trevor about projects\\n\\nwe could work on together. I missed working with them, and it seemed', metadata={'essay': 'worked', 'source': '../data/PaulGrahamEssaysLarge/worked.txt'})]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.get_relevant_documents(\"Tell me about investment advice the 'worked' essay? return only 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e28e68",
      "metadata": {
        "id": "e9e28e68"
      },
      "source": [
        "Awesome! It returned it back for us. It's a bit rigid because you need to put in the exact name of the file/essay you want to get. You could make a pre-step and infer the correct essay from the users choice but this is out of scope for now and application specific."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}